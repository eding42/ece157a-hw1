{"cells":[{"cell_type":"markdown","metadata":{"id":"ZpauiZXHuxQI"},"source":["# Overview\n","This notebook is used to create a regression model that predicts the median house price within a block based on a number of features. The dataset used is the \"California Housing Prices\" dataset which is available at https://www.kaggle.com/datasets/camnugent/california-housing-prices/data. Here is a summary of the Kaggle description of the dataset:\n","\n","The dataset discussed is initially featured in the paper \"Sparse Spatial Autoregressions\" by R. Kelley Pace and Ronald Barry, published in the Statistics & Probability Letters, 33 (1997) 291-297. The author of the Kaggle project encountered this dataset in the second chapter of Aurélien Géron's book, \"Hands-On Machine Learning with Scikit-Learn and TensorFlow.\" This dataset was a modified version of the California Housing dataset available at Luís Torgo's page (University of Porto) http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html.\n","    \n","This dataset contains information about houses in California districts derived from the 1990 California census. While it may not be suitable for predicting current housing prices, it serves as a great introduction for building machine learning models due to its need for basic data cleaning, a clear and easily comprehensible list of features, and an optimal size that strikes a balance between being overly simplistic and excessively complex.\n","\n","The dataset contains 20640 rows and 10 columns. The columns are as follows:\n","1. longitude (float): A measure of how far west a house is; a higher value is farther west\n","2. latitude (float): A measure of how far north a house is; a higher value is farther north\n","3. housing_median_age (float): Median age of houses within a block; a lower number is a newer building\n","4. total_rooms (float): Total number of rooms within a block\n","5. total_bedrooms (float): Total number of bedrooms within a block\n","6. population (float): Total number of people residing within a block\n","7. households (float): Total number of households, a group of people residing within a home unit, for a block\n","8. median_income (float): Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n","9. median_house_value (float, target): Median house value for households within a block (measured in US Dollars)\n","10. ocean_proximity (string): Location of the house w.r.t. ocean/sea\n","\n","This notebook performs the following steps to create a regression model:\n","1. Data Exploration\n","2. Data Preprocessing\n","3. Model Training and Evaluation\n","\n","The creation of this notebook has taken inspiration from the following sources:\n","- Aurélien Géron's \"02_end_to_end_machine_learning_project.ipynb\" python notebook in \"handson-ml2\" git repository. https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb\n","- Andrey Shtrauss's \"Bayesian Regression | House Price Prediction\" python notebook in Kaggle. https://www.kaggle.com/code/shtrausslearning/bayesian-regression-house-price-prediction#5-|-FEATURE-ENGINEERING\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U-elrkqTuxQR"},"source":["# Install Python Packages/Libraries"]},{"cell_type":"markdown","metadata":{"id":"DX1Zk5icuxQS"},"source":["Install the specified Python packages. Here's a breakdown of each package:\n","\n","`joblib`: A set of tools for pipelining Python jobs. It provides utilities for saving and loading Python objects that make it possible to save scikit-learn models in a format that can be used in production.\n","\n","`matplotlib`: A plotting library for creating visualizations in Python. It is often used in conjunction with other libraries for data analysis and machine learning.\n","\n","`numpy`: A fundamental package for scientific computing with Python. It provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays.\n","\n","`pandas`: A powerful data manipulation and analysis library. It provides data structures for efficiently storing and manipulating large datasets.\n","\n","`seaborn`: A statistical data visualization library based on Matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics.\n","\n","`scikit-learn`: A machine learning library that provides simple and efficient tools for data analysis and modeling. It includes various algorithms for classification, regression, clustering, and more.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLXq21oCuxQS"},"outputs":[],"source":["%pip install joblib==1.3.2 matplotlib==3.7.1 numpy==1.23.5 pandas==1.5.3 plotly==5.15.0 scikit-learn==1.2.2 seaborn==0.13.1 \"nbformat>=4.2.0\""]},{"cell_type":"markdown","metadata":{"id":"thuntT1JuxQT"},"source":["# Import Packages/Libraries"]},{"cell_type":"markdown","metadata":{"id":"trlV-tCPuxQT"},"source":["In addition to the packages/libraries installed above, we will also imported\n","\n","`typing`: A module that provides support for type hints. Type hints allow you to specify the type of a variable, function parameter, or return value. This helps improve the readability of your code and allows you to catch errors early."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MymLVTZtuxQU"},"outputs":[],"source":["from typing import List, Tuple\n","import zipfile\n","import os\n","import joblib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import plotly.express as px\n","import seaborn as sns\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import KNNImputer\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n"]},{"cell_type":"markdown","metadata":{"id":"xE1Kh65yuxQU"},"source":["# Data Exploration\n","\n","We want to explore the data to get a better understanding of the dataset. We will use the pandas library to load the dataset into a pandas DataFrame. A DataFrame is a two-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data, and more) in columns. It is similar to a spreadsheet or an SQL table. The DataFrame object also has powerful built-in methods for exploring and manipulating these data sets. We will first take a look at the structure of the dataset. Then, we want to explore the data to see if there are any missing values and to see if there are any skewness and outliers. We also want to see if there are any correlations between the features and the target variable. We will use the pandas library to load the dataset and the matplotlib, seaborn, and plotly library to plot the data."]},{"cell_type":"markdown","metadata":{"id":"RrVyMfuiuxQU"},"source":["Load the `housing.csv` file into a Pandas DataFrame using the `read_csv()` function. The `read_csv()` function takes in the path to the CSV file as a parameter and returns a DataFrame."]},{"cell_type":"code","source":["!unzip dataset.zip -d dataset # Unzip the file if needed."],"metadata":{"id":"Ukh60OOQwuHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeILh2REuxQU"},"outputs":[],"source":["csv_path =  # TODO: create variable for dataset path\n","df =  # TODO: read dataset into dataframe"]},{"cell_type":"markdown","metadata":{"id":"MPycXYsDuxQU"},"source":["Display the first 5 rows of the dataframe using `head()` method of the dataframe.\n","\n","We can see that the data contains 10 columns where the first 9 columns are numerical (continuous) values and the last column is a string (categorical) value. The target variable is the `median_house_value` column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMnCLuoFuxQV"},"outputs":[],"source":["# TODO: show the first 5 rows of dataframe\n"]},{"cell_type":"markdown","metadata":{"id":"0IYcBdk0uxQV"},"source":["Obtain a concise summary of the dataframe. The `info()` method provides information about the dataframe, including the index range, the data types of each column, the number of non-null values, and memory usage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwVrdvEKuxQV"},"outputs":[],"source":[" # TODO: show the info of dataframe"]},{"cell_type":"markdown","metadata":{"id":"xxz6WvBSuxQV"},"source":["Retrieve the columns of the dataframe using the `columns` attribute of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"er77LHDPuxQV"},"outputs":[],"source":[" # TODO: show the columns of dataframe"]},{"cell_type":"markdown","metadata":{"id":"w_ES3_VAuxQV"},"source":["Retrieve the dimensions (rows, columns) of the dataframe using the `shape` attribute of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ce1uT95uxQV"},"outputs":[],"source":["# TODO: retrieve the number of rows and columns\n"]},{"cell_type":"markdown","metadata":{"id":"ThNxggmZuxQW"},"source":["Check and handles missing values in the Pandas DataFrame using the `isnull()` method of the dataframe. The `isnull()` method returns a boolean dataframe where each cell is either True or False depending on whether the corresponding cell in the dataframe is null or not. We can then use the `sum()` method to count the number of null values in each column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFh_VBVbuxQW"},"outputs":[],"source":["# TODO: check the number of missing values in each column\n","print('Missing values in each column:\\n', , sep='')\n","print('Original data shape: ', df.shape)\n","print('='*25)\n","\n","# TODO: print the rows with missing values and store the row indexes\n","null_rows =\n","null_rows_index =\n","print('Rows with missing values:\\n')\n","null_rows\n"]},{"cell_type":"markdown","metadata":{"id":"3agEJKbBuxQW"},"source":["We have a few options to handle missing values:\n","- Option 1: We can drop the rows or columns that contain missing values using the `dropna()` method of the dataframe. The `dropna()` method takes in the axis to drop as a parameter. We can use the `axis` parameter to specify whether to drop rows or columns. By default, `axis=0` which drops the rows.\n","- Option 2: We can replace the missing values with the mean, median, or mode of the column. We will replace the missing values with the median of the column. We can use the `fillna()` method of the dataframe to replace the missing values with the median of the column. The `fillna()` method takes in the value to replace the missing values with as a parameter. We can use the `median()` method of the dataframe to obtain the median of the column.\n","- Option 3: We can replace the missing values with KNN imputation. KNN imputation is a method that uses the k-nearest neighbors algorithm to impute missing values. It works by finding the k-nearest neighbors of data points with missing values and then imputing the missing values with the mean of the neighbors. We will use the `KNNImputer()` from the `sklearn.impute` module to impute the missing values. The `KNNImputer()` takes in the number of neighbors and weights to use as a parameter. We will use 10 neighbors and weight points by the inverse of their distance. We can then use the `fit_transform()` method of the `KNNImputer()` object to impute the missing values. The `fit_transform()` method takes in the dataframe as a parameter and returns a numpy array. We can then convert the numpy array to a dataframe using the `DataFrame()` from the `pandas` library. Reference link for `KNNImputer()`: https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLyd0y6suxQW"},"outputs":[],"source":["# TODO: Option 1: drop the rows with missing values\n","df_dropped_na =\n","print('Missing values after dropped na:\\n', df_dropped_na.isnull().sum(), sep='')\n","print('Data shape after dropping nan:', df_dropped_na.shape)\n","print('='*25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKFmmAcRuxQW"},"outputs":[],"source":["# TODO: Option 2: fill in the missing values with the median of the column\n","df_median_filled =\n","print('Missing values after filling with median values:\\n', df_median_filled.isnull().sum(), sep='')\n","print('Data shape after filling with median values:', df_median_filled.shape)\n","\n","# check the null rows again\n","print('Median values for each column:\\n', df.median(), sep='')\n","df_median_filled.iloc[null_rows_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw4SadkBuxQW"},"outputs":[],"source":["# Option 3: use K-nearest neighbor imputation to fill in the missing values\n","# see https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n","# for more details and parameters on the KNNImputer\n","imputer =  # TODO: create a KNNImputer object\n","\n","# TODO: separate the categorical and numerical columns as\n","# KNNImputer can only handle numerical columns\n","cat_cols =\n","num_cols =\n","\n","# TODO: impute the missing values in the numerical columns\n","# the imputer returns a numpy array with imputed values\n","df_num_filled =\n","\n","# TODO: convert the imputed numpy array back to a pandas dataframe\n","df_num_filled =\n","\n","# TODO: concatenate the imputed numerical columns with the categorical columns\n","df_filled =\n","print('Missing values after KNN impute:\\n', df_filled.isnull().sum(), sep='')\n","print('Data shape after KNN impute:', df_filled.shape)\n","\n","# check the null rows again\n","df_filled.iloc[null_rows_index]"]},{"cell_type":"markdown","metadata":{"id":"zPtwtHiCuxQX"},"source":["Check the number of unique values in each column of dataframe using the `nunique()` method of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49YdfBAKuxQX"},"outputs":[],"source":["# Check the number of unique values in each column\n","df.nunique()"]},{"cell_type":"markdown","metadata":{"id":"enWByYKjuxQX"},"source":["From the `df.info()` and `df.head(5)` cell, we know that `ocean_proximity` is an object/string data type. We can use the `value_counts()` method to count the occurrences of each unique value in the column named `ocean_proximity`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbML7h5XuxQX"},"outputs":[],"source":["# Check the occurences of each unique value in the 'ocean_proximity' column\n","df['ocean_proximity'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"4TRHOcZRuxQX"},"source":["We can also display a pie chart to visualize the distribution of values in the `ocean_proximity` column using the `plot.pie()` method of the Pandas Series."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8oh8hwDFuxQX"},"outputs":[],"source":["# Create a plt figure. Set the figure size and background color to white in case your editor GUI uses a dark theme\n","plt.figure(figsize=(5, 5), facecolor='white')\n","\n","# Show the percentage of each category using a pie chart\n","# plot.pie() plots a pie chart on the figure attached to the current cell\n","# autopct='%1.2f%%' shows the percentage of each category with 2 decimal places\n","df['ocean_proximity'].value_counts().plot.pie(autopct='%1.2f%%')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VAslRiG2uxQX"},"source":["Generate descriptive statistics of the numerical columns in the dataframe using the `describe()` method of the dataframe. The `describe()` method computes summary statistics of numerical columns, including count, mean, standard deviation, minimum, maximum, and the 25, 50, and 75 percentiles."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCMF_vj_uxQY"},"outputs":[],"source":["# Get the summary statistics of the numerical columns\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"iZwjD6ULuxQY"},"source":["Visualize the distribution of the numerical columns in the dataframe using the `hist()` method of the dataframe. The `hist()` method creates a histogram for each numerical column in the dataframe. A histogram shows the frequency of values in a given range. The x-axis represents the value range and the y-axis represents the frequency of values in that range.\n","\n","The histograms show that the features have very different scales. We will need to scale the features before training the model to avoid biasing the model towards features with larger values. We also see that some features (e.g. total_rooms, total_bedrooms, population, and households) are skewed due to the presence of outliers. We will transform the features using logarithmic scaling to reduce the skewness. We will perform these steps in the preprocessing section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2g7q6SFZuxQY"},"outputs":[],"source":["# Plot the distribution for all numerical columns\n","# bins=50 means to divide the range of values into 50 equal-sized bins\n","# figsize=(20, 15) means to set the figure size to width=20 and height=15\n","df.hist(bins=50, figsize=(20, 15))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"y5jR08AhuxQY"},"source":["Check for correlation between the features and the target variable using the `corr()` method of the dataframe. The `corr()` method computes pairwise correlation of columns, excluding NA/null values. The correlation coefficient ranges from -1 to 1. When it is close to 1, it means that there is a strong positive correlation; when the coefficient is close to -1, it means that there is a strong negative correlation; when it is close to zero, it means that there is no linear correlation.\n","\n","Looking at the last row, we can see that the `median_house_value` has a strong positive correlation with the `median_income` (0.69). We can also see that the `median_house_value` has a weak correlation with the `longitude` (-0.05), `total_bedrooms` (0.05), `population` (-0.02), and `households` (0.07). We will consider removing the weakly correlated features in the preprocessing section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKEqSeuvuxQY"},"outputs":[],"source":["# Create a figure\n","plt.figure(figsize=(12, 8))\n","\n","# Compute the correlation matrix using the `.corr()` method\n","correlation_matrix = df.corr()\n","\n","# Create a mask to block the upper triangle of the correlation matrix\n","# as it is a mirror image of the lower triangle\n","# use `np.triu()` to create an upper triangle matrix of True and False\n","mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n","\n","# Plot the heatmap using seaborn's `heatmap()` function\n","# set `annot=True` to show the correlation values on the heatmap\n","# set `fmt='.2f'` to round the correlation values to 2 decimal places\n","# set `mask=mask` to block the upper triangle of the correlation matrix\n","sns.heatmap(correlation_matrix, annot=True, fmt='.2f', mask=mask)\n","\n","# show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ltfAxogXuxQZ"},"source":["The correlation matrix can also be viewed as a dataframe to see the correlation coefficients of all the features with each other. Execute the cell below to view the correlation matrix as a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gP1DfOJuxQZ"},"outputs":[],"source":["correlation_matrix"]},{"cell_type":"markdown","metadata":{"id":"pOQ-lIWYuxQZ"},"source":["We can extract the correlation coefficients of the features with the target variable by selecting the column named `median_house_value` from the correlation matrix dataframe. We have sorted that column in descending order to see the features with the highest correlation with the target variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vARirPVWuxQZ"},"outputs":[],"source":["house_value_corr = correlation_matrix['median_house_value'].sort_values(ascending=False)\n","print(house_value_corr)"]},{"cell_type":"markdown","metadata":{"id":"a4gTdCPIuxQa"},"source":["View the correlation between the numerical columns in a Pandas DataFrame using scatter plots. This allows us to visualize linear and non-linear relationships between the features and target if any exists. The plots are generated using the `pairplot()` method of seaborn library. We are using it just to plot the numerical columns against the `median_house_value` column. You can also use the `pairplot()` method to plot all the columns in the dataframe by removing the `x_vars` and `y_vars` parameters from the method. `plot_kws` is a dictionary of keyword arguments that are passed to the scatter plot function inside `pairplot()` method. We are using it to set `alpha` to 0.5 to make the points more transparent.\n","\n","We can see that the scatter plot for `median_income` and `median_house_value` show a strong positive correlation where the points are have a linear relationship. While the scatter plots for `housing_median_age`, `longitude`, and `latitude`,  show a weak correlation where the points are spread across the plot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qc_zHsjluxQa"},"outputs":[],"source":["# Plot the scatter plot between column pairs\n","sns.pairplot(df, x_vars=house_value_corr.index, y_vars=['median_house_value'], height=5, plot_kws={'alpha': 0.5})"]},{"cell_type":"markdown","metadata":{"id":"OMD_jkvyuxQa"},"source":["Check the correlation between the location of the houses and the median house value via longitude and latitude. We will use the `plotly.express` library to plot the geographical data. The `scatter_mapbox()` function creates a scatter plot on a map. The `px.scatter_mapbox()` function takes the dataframe as the first argument and the column names of the latitude and lonitude columns as the second and third arguments. The `color` parameter is used to specify the column to be used for coloring the points, which is the median house value in this case. The `hover_data` parameter is used to specify the column data to show when the mouse hover over a pont. The `mapbox_style` parameter is used to specify the style of the map.\n","\n","This plot is interactive. You can zoom in and out of the map, pan around, and hover over the points to see the data values.\n","\n","We can see that there are high density of expensive houses in the Bay Area and Los Angeles area, and also a relatively dense expensive houses in the San Diego area. We can see that as we move away from the coast, the house prices decrease. So, we can use the `ocean_proximity` column as a feature to predict the house prices. Additionally, we can create a new feature by using the `longitude` and `latitude` columns to compute the distance from the closest dense areas of expensive houses. This can be done by simply picking a reference point (anchor) from each of the dense expensive area, and computing a given point's distance from the closest anchor. We will perform these steps in the preprocessing section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IQ2Ox0GuxQa"},"outputs":[],"source":["# Use plotly express to plot the scatter plot on a map\n","fig = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\", color=\"median_house_value\",\n","                        width=800, height=600,\n","                        hover_data=df.columns,\n","                        mapbox_style=\"open-street-map\",\n","                        zoom=5,\n","                        )\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"OuODFBSDuxQa"},"source":["# Data Preprocessing\n","\n","Data splitting. We are given one CSV file containing all the data. We will split the data into training, validation, and test sets.\n","- The training dataset is used to train the machine learning model. The model learns the patterns and relationships within the data from this set. The model adjusts its parameters to minimize the difference between its predictions and the ground truth values provided with this dataset.\n","- The validation dataset is used to assess the model's performance during training and guide the adjustment of the hyperparameters. Hyperparameters are training/model configurations that the programmer can manually adjust. Since the validation dataset is an independent dataset not used in directly tuning the model's internal parameters, we can use it to assess whether a model is under-fitting or over-fitting.\n","- The test dataset is kept separate from both the training and validation sets. It is used to assess the final performance of the trained model on unseen data. The test set provides an unbiased evaluation of the model's generalization to new, previously unseen examples.\n","\n","Building pipelines to preprocess the data. The preprocessing pipeline provides a systematic and efficient way to streamline and automate the data preprocessing steps. It ensures consistent application of preprocessing steps to training, validation, and test data, enhancing model reproducibility, readability, and ease of deployment. In this section, we will build a preprocessing pipeline to perform the following steps:\n"," - Fill nan values with nearest neighbors\n"," - Scale the data using logarithmic transformation to reduce the skewness of the data\n"," - Standardize features by removing the mean and scaling to unit variance using StandardScaler\n"," - Encode string categorical features using OneHotEncoder\n"," - Compute new feature based on latitude and longitude (distance from dense area)\n"," - Drop low correlation features\n"]},{"cell_type":"markdown","metadata":{"id":"CW7wj8-guxQa"},"source":["Reload the data into a Pandas DataFrame to reset the changes made to the dataframe in the previous section. Additionally, we will declare variables to store the names of the numerical, categorical, and label (target) columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvc-NCLbuxQb"},"outputs":[],"source":["# TODO: reload the dataset\n","csv_path =\n","df =\n","\n","# TODO: declare the categorical, numerical, and target column\n","cat_features =\n","num_features =\n","all_features = cat_features + num_features\n","\n","label ="]},{"cell_type":"markdown","metadata":{"id":"nCAk7nZYuxQb"},"source":["Split the data into training, validation, and test sets. The training set will be used to train the model, the validation set will be used to evaluate the model during the training process, and the test set will be used to test the final model.\n","\n","We will be using Scikit-Learn's `train_test_split` function to split the data into training, validation, and test sets. The `train_test_split` function takes in the dataframe as argument and returns the training set and test set. We will further split the training set into training and validation sets. The final split is 60% training, 20% validation, and 20% test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DxlC6KyuxQb"},"outputs":[],"source":["# TODO: use a random seed so that we can reproduce the results\n","# this is important when you want to compare different models in different executions\n","random_seed =\n","\n","# TODO: split the data into training and test sets. 80/20 split.\n","train_set, test_set =\n","\n","# TODO: split the data into training and validation sets.\n","# 75/25 split of the training set, which is 60/20 of the original set.\n","train_set, valid_set ="]},{"cell_type":"markdown","metadata":{"id":"qLBbvQd4uxQb"},"source":["Separate the feature columns from the label column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GW4CvZs5uxQb"},"outputs":[],"source":["# TODO: separate the features and labels in the training, validation, and test sets\n","train_X, train_y =\n","valid_X, valid_y =\n","test_X, test_y ="]},{"cell_type":"markdown","metadata":{"id":"5R47_6k7uxQb"},"source":["Check the feature distribution in the training, validation, and test sets. We want to ensure that the distribution is similar in all the sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyvEAUbjuxQb"},"outputs":[],"source":["# Check the distribution of the training set, validation set, and test set\n","# For each feature, overlay the histograms of the three sets on the same subplot\n","\n","# TODO: create a figure with 3 rows and 3 columns\n","fig, axes =\n","\n","# TODO: for each feature, plot the histograms of the three sets on the same subplot\n","for ax, col in zip(, ):\n","\n","    ax.set_title(col)\n","\n","# TODO: add a legend to the figure anchor to the top right corner\n","handles, labels = ax.get_legend_handles_labels()\n","fig.legend(handles, labels, bbox_to_anchor=)\n","\n","# TODO: use `plt.tight_layout()` to adjust the padding between and around subplots.\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ktm5dXtbuxQc"},"source":["Check the label distribution in the training, validation, and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZcV_tq9uxQc"},"outputs":[],"source":["# TODO: check the label distribution of the training set, validation set, and test set\n","# overlay the histograms of the three sets\n","\n","plt.figure(figsize=(10, 5))\n","\n","plt.title(label)\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"CAEIRrFvuxQc"},"source":["## Create Preprocessing Pipeline"]},{"cell_type":"markdown","metadata":{"id":"bF5H79k7uxQc"},"source":[" - Fill nan values with nearest neighbors\n"," - Scale the data using logarithmic transformation to reduce the skewness of the data\n"," - Standardize features by removing the mean and scaling to unit variance using StandardScaler\n"," - Encode string categorical features using OneHotEncoder\n"," - Compute new feature based on latitude and longitude (distance from dense area)\n"," - Drop low correlation features"]},{"cell_type":"markdown","metadata":{"id":"MwADVnJUuxQc"},"source":["Below we will create some custom data transformers, which will be used in the preprocessing pipeline. These custom transformers inherits `BaseEstimator` and `TransformerMixin`. These classes are provided by scikit-learn to help in creating custom transformers and estimators that seamlessly integrate with scikit-learn's pipeline.\n","\n","`BaseEstimator` is the base class for all scikit-learn estimators. An estimator is an object that fits a model based on some training data and is capable of inferring some properties on new data. It can be, for instance, a classifier or a regressor. All estimators implement the fit method (`fit(X, y)`).\n","\n","`TransformerMixin` is a mixin class for transformers in scikit-learn. A transformer implements the `fit_transform` methods. It provides a default implementation for the `fit_transform` method, which combines fitting and transforming in a single step."]},{"cell_type":"markdown","metadata":{"id":"uFgvjo9juxQc"},"source":["Custom feature engineering class, `MinDistanceFromAnchors`, finds the nearest anchor (based on latitude and longitude information)\n","        for each given data point and output their Euclidean distance. This transformer is designed to be used within a scikit-learn preprocessing pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyTgbrk2uxQc"},"outputs":[],"source":["# Feature Engineering to create new features based on latitude and longitude\n","class MinDistanceFromAnchors(BaseEstimator, TransformerMixin):\n","    def __init__(self) -> None:\n","        '''Finds the nearest anchor (based on latitude and longitude information)\n","        for each given data point and output their Euclidean distance\n","        '''\n","        self.anchors = [(37.38, -122.21), # Bay Area\n","                        (33.99, -118.5), # Los Angeles\n","                        (32.82, -117.31), # San Diego\n","                        ]\n","        self.num_anchors = len(self.anchors)\n","\n","    def fit(self, X: np.ndarray, y: np.ndarray=None) -> 'MinDistanceFromAnchors':\n","        return self\n","\n","    def transform(self, X: np.ndarray) -> np.ndarray:\n","        '''Calculate the euclidean distance between each data point and each anchor,\n","        store the distances in the distances matrix, and filter to nearest anchor distance only.\n","        '''\n","\n","        # initialize the distances matrix\n","        distances = np.zeros((X.shape[0], self.num_anchors))\n","\n","        # calculate the euclidean distance between each data point and each anchor\n","        for i in range(self.num_anchors):\n","            lat_long_points = X.astype(float)\n","            anchor = np.array(self.anchors[i])\n","            distances[:, i] = np.sqrt(np.sum((lat_long_points - anchor)**2, axis=1))\n","\n","        # find the minimum distance out of the distances to the three anchors\n","        min_distances = np.min(distances, axis=1)\n","\n","        return min_distances.reshape(-1, 1)\n"]},{"cell_type":"markdown","metadata":{"id":"nNOOQv8VuxQc"},"source":["`LogScaler` is a custom transformer that scales the data using logarithmic transformation to reduce the skewness of the data. The `FunctionTransformer` class is used to create a transformer that applies a function to the data. The `FunctionTransformer` class takes in the function to be applied as the first argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6R2CTzwuxQd"},"outputs":[],"source":["# TODO: create a custom transformer using np.log() to log transform the data\n","log_scaler ="]},{"cell_type":"markdown","metadata":{"id":"WSUFVxTwuxQd"},"source":["Sets up four preprocessing pipelines. The pipelines consist of several sequential steps, each represented by a tuple with a name and a corresponding transformer.\n","1. `lat_long_pipeline` is a pipeline for the `latitude` and `longitude` columns. It consists of the `MinDistanceFromAnchors` transformer, which calculates the Euclidean distance from data points to their nearest anchor based on latitude and longitude information. The `StandardScaler` transformer is used to standardize the data by removing the mean and scaling to unit variance.\n","2. `log_std_pipeline` is a pipeline for the numerical columns with skew distribution. It consists of the `KNNImputer` to fill in any missing values. The `LogScaler` transformer, which scales the data using logarithmic transformation to reduce the skewness of the data. The `StandardScaler` transformer is used to standardize the data by removing the mean and scaling to unit variance.\n","3. `std_pipeline` is a pipeline for the remaining numerical columns. It consists of the `KNNImputer` to fill in any missing values. The `StandardScaler` transformer is used to standardize the data by removing the mean and scaling to unit variance.\n","4. `cat_pipeline` is a pipeline for the categorical columns. It consists of the `OneHotEncoder` transformer, which encodes categorical features as a one-hot numeric array. `sparse=False` is used to return a dense array representation in numpy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJt_HOvmuxQd"},"outputs":[],"source":["# Latitude and longitude preprocess pipeline\n","lat_long_pipeline = Pipeline([\n","    ('min_distance_from_anchors', MinDistanceFromAnchors()),\n","    ('std_scaler', StandardScaler()),\n","])\n","\n","# Create the preprocess pipeline for log scaling and standard scaling features\n","log_std_pipeline = Pipeline([\n","        ('imputer', KNNImputer(n_neighbors=10, weights='distance')),\n","        ('log_scaler', log_scaler),\n","        ('std_scaler', StandardScaler()),\n","    ])\n","\n","# Create the preprocess pipeline for standard scaling features\n","std_pipeline = Pipeline([\n","        ('imputer', KNNImputer(n_neighbors=10, weights='distance')),\n","        ('std_scaler', StandardScaler()),\n","    ])\n","\n","# Create the preprocess pipeline for categorical features\n","cat_pipeline = Pipeline([\n","        ('one_hot_encoding', OneHotEncoder(sparse=False, categories=[['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR OCEAN', 'NEAR BAY']])),\n","    ])"]},{"cell_type":"markdown","metadata":{"id":"BEPt2q5kuxQd"},"source":["We will test out the pipelines on the training set. We will use the `fit_transform` method to fit the pipeline to the training set and transform the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuoITr5puxQd"},"outputs":[],"source":["# make a copy of the training set to test the preprocess pipelines\n","train_X_copy = train_X.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TcxzUv6suxQd"},"outputs":[],"source":["# test out the lat_long_pipeline\n","lat_log_X = train_X_copy[['latitude', 'longitude']]\n","distances = lat_long_pipeline.fit_transform(lat_log_X)\n","\n","# add the new feature to the training set\n","train_X_copy['distance_from_anchor'] = distances\n","\n","# using plotly express to plot the scatter plot on a map using the new feature as the color\n","fig = px.scatter_mapbox(train_X_copy, lat=\"latitude\", lon=\"longitude\", color=\"distance_from_anchor\",\n","                        width=800, height=600,\n","                        hover_data=train_X_copy.columns,\n","                        mapbox_style=\"open-street-map\",\n","                        zoom=5,\n","                        )\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzLN8p4TuxQd"},"outputs":[],"source":["# testing out the log_std_pipeline\n","log_std_X = train_X_copy[['total_rooms']]\n","log_std_X_transformed = log_std_pipeline.fit_transform(log_std_X)\n","\n","# add the new features to the training set\n","train_X_copy['log_total_rooms'] = log_std_X_transformed[:, 0]\n","\n","# plot the histograms of the new features\n","train_X_copy[['total_rooms']].hist(figsize=(20,5), bins=50, layout=(1,4))\n","plt.show()\n","train_X_copy[['log_total_rooms']].hist(figsize=(20,5), bins=50, layout=(1,4))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OX3K84CDuxQd"},"outputs":[],"source":["# testing out the std_pipeline\n","log_std_X = train_X_copy[['housing_median_age', 'median_income']]\n","log_std_X_transformed = std_pipeline.fit_transform(log_std_X)\n","\n","# add the new features to the training set\n","train_X_copy['std_housing_median_age'] = log_std_X_transformed[:, 0]\n","train_X_copy['std_median_income'] = log_std_X_transformed[:, 1]\n","\n","# plot the histograms of the new features\n","train_X_copy[['housing_median_age', 'median_income']].hist(figsize=(20,5), bins=50, layout=(1,2))\n","plt.show()\n","train_X_copy[['std_housing_median_age', 'std_median_income']].hist(figsize=(20,5), bins=50, layout=(1,2))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgl0-XfzuxQe"},"outputs":[],"source":["# test out the cat_pipeline\n","cat_X = train_X_copy[['ocean_proximity']]\n","cat_X_transformed = cat_pipeline.fit_transform(cat_X)\n","\n","# print out the first five rows with the categories\n","print('Value before transformation:')\n","print(cat_X[:5], end='\\n==================\\n')\n","\n","print('Value after transformation:')\n","print(cat_pipeline['one_hot_encoding'].categories_)\n","print(cat_X_transformed[:5])"]},{"cell_type":"markdown","metadata":{"id":"TH7L76Q6uxQe"},"source":["Combine all the pipelines using `ColumnTransformer`. `ColumnTransformer` allows us to apply different preprocessing steps to different columns in the dataset. It is particularly useful when we have a dataset with data types or when we want to apply specific transformations to specific subsets of features.\n","\n","`ColumnTransformer` takes in a list of tuples, where each tuple contains a name, a transformer, and a list of columns to which the transformer should be applied. The `remainder` parameter is set to `'passthrough'` to ensure that the columns not specified in the list of tuples are passed through without any transformations. Or, we can set `remainder` to `'drop'` to drop the columns not specified in the list of tuples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIcJRMzfuxQe"},"outputs":[],"source":["# TODO: create a pipeline with the ColumnTransformer\n","preprocess_pipeline = ColumnTransformer([\n","\n","\n","\n","\n","    ], remainder='drop')"]},{"cell_type":"markdown","metadata":{"id":"Kcbi0ucHuxQe"},"source":["Use the full pipeline to fit on the training data, then transform the training and validation data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CN2DGloguxQe"},"outputs":[],"source":["# TODO: fit the pipeline to the training set and transform it\n","train_X =\n","\n","# TODO: transform the validation set\n","valid_X =\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtA_Nh56uxQe"},"outputs":[],"source":["# quick check on the features of the training set\n","# should contains features `distance from anchor`, `total_rooms`,\n","# `housing_median_age`, `median_income`, and five features from one-hot `ocean_proximity`.\n","pd.DataFrame(train_X).hist(bins=50, figsize=(20, 15))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SuQR9vZsuxQf"},"source":["# Train and Evaluate Models"]},{"cell_type":"markdown","metadata":{"id":"8Ol_VmwYuxQf"},"source":["This perhaps is the easier part of the workflow in terms of coding. Scikit-learn provides a wide range of machine learning tools that we can use to train and evaluate on our data. Their APIs are very consistent. We can use the same code to train and evaluate different models by simply changing the model class."]},{"cell_type":"markdown","metadata":{"id":"paljhVsfuxQf"},"source":["Before we start, we will implement an evaluation function. This function `predict_and_print_metrics` takes a regression model, input features (X), target values (y), and a dataset name. It then predicts the target values using the model, calculates and prints the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE).\n","\n","MAE (Mean Absolute Error):\n","- MAE is calculated as the mean of absolute differences between predicted and true values.\n","- It treats all errors equally, regardless of their magnitude.\n","- $MAE = \\frac{1}{N}\\sum_{i=1}^{N} |y_i - \\hat{y_i}|$ where $y_i$ is true value of sample $i$, $\\hat{y_i}$ is the model prediction value of sample $i$, ad $N$ is the number of samples\n","\n","RMSE (Root Mean Squared Error):\n","- RMSE is calculated as the square root of the mean of squared differences between predicted and true values.\n","- It penalizes large errors more heavily than small ones.\n","- $RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y_i})^2}$ where $y_i$ is true value of sample $i$, $\\hat{y_i}$ is the model prediction value of sample $i$, ad $N$ is the number of samples\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asoJncEruxQf"},"outputs":[],"source":["# TODO: import the mean squared error and mean absolute error metrics\n","\n","\n","# TODO: create a function to predict and print the metrics\n","def predict_and_print_metrics(model, X: np.ndarray, y: np.ndarray, name: str) -> None:\n","    # predict the target values\n","    y_pred =\n","\n","    # compute and print the metrics\n","    print(f'{name} MAE: {}')\n","    print(f'{name} RMSE: {}')\n"]},{"cell_type":"markdown","metadata":{"id":"l69iAzPjuxQf"},"source":["Train a Linear Regression model. The training can be done in three lines of code. First, we import the model class from the Scikit-Learn library. Then, we create an instance of the model class. Third, we call the `fit` method of the model instance to train the model. The `fit` method takes in the training features and labels as arguments. The model learns the patterns and relationships within the data. Then, we use the trained model to predict the target values for the training and validation set, and we evaluate the model using the evaluation metrics mentioned above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guKID3_ouxQf"},"outputs":[],"source":["# TODO: import the linear regression model\n","\n","\n","# TODO: instantiate and train the model\n","lin_reg =\n","\n","# TODO: evaluate the model on the training set and validation set"]},{"cell_type":"markdown","metadata":{"id":"PPoagP5duxQf"},"source":["We will train and evaluate another model (K-neighbors regressor). We will use the same code to train and evaluate different models by simply changing the model class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMONlh8XuxQf"},"outputs":[],"source":["# TODO: import the K-neighbors regressor\n","\n","\n","# TODO: instantiate and train the model\n","neigh =\n","\n","# TODO: evaluate the model on the training set and validation set"]},{"cell_type":"markdown","metadata":{"id":"FVJVslfsuxQg"},"source":["In the case where the model has hyperparameters, we can either write our own code to search for the best hyperparameters or use Scikit-Learn's `GridSearchCV` class to search for the best hyperparameters. We start off by writing our own code to search for the best hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KT8V0VZpuxQg"},"outputs":[],"source":["# Train a k-neighbors regressor with search for the best hyperparameters\n","\n","# TODO: create lists to store the train and validation scores\n","train_scores =\n","valid_scores =\n","\n","# TODO: create a list of hyperparamters to try out\n","neighbors =\n","\n","# TODO: loop through the parameters and fit the model\n","# evaluate the model on the train set and validation set\n","for neighbor in neighbors:\n","    neigh =\n","\n","    # evaluate the model on the train set\n","    train_y_pred =\n","    train_mae =\n","    train_scores.append(train_mae)\n","\n","    # evaluate the model on the validation set\n","    valid_y_pred =\n","    valid_mae =\n","    valid_scores.append(valid_mae)\n","\n","# TODO: plot the learning curves\n","plt.plot(, , label=)\n","plt.plot(, , label=)\n","plt.xlabel('number of neighbors')\n","plt.ylabel('MAE')\n","plt.legend()\n","plt.show()\n","\n","# TODO: print the neighbors and the corresponding scores\n","for neighbor, train_score, valid_score in zip(, , ):\n","    print(f'neighbors: {neighbor}, train score: {train_score}, valid score: {valid_score}')"]},{"cell_type":"markdown","metadata":{"id":"h7ugKAn5uxQg"},"source":["Use `GridSearchCV` to search for the best hyperparameters. `GridSearchCV` takes in the model class, hyperparameter grid, evaluation metric (scorer), and number of folds as arguments. It then searches for the best hyperparameters based on the evaluation metric. We will use the same evaluation metric as before, MAE. We will combine the training and evaluation data into one dataset. This is because `GridSearchCV` uses cross-validation to evaluate the model. We will use 5-fold cross-validation, which means that the dataset will be split into 5 folds. The model will be trained and evaluated 5 times, each time using a different fold as the evaluation set. The final evaluation metric will be the average of the 5 evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AsLRMMpruxQg"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer\n","\n","# TODO: combine the train and validation sets for GridSearchCV\n","train_valid_X =\n","train_valid_y =\n","\n","# TODO: Specify the parameter grid for the grid search\n","param_grid = {\n","\n","}\n","\n","# TODO: Define your MAE evaluation metric function\n","def custom_metric(y_true, y_pred):\n","    # Implement your custom metric calculation here\n","    # This example uses the mean absolute error (MAE) as a placeholder\n","    mae = mean_absolute_error(y_true, y_pred)\n","    return mae\n","\n","# TODO: Make the metric a scorer using make_scorer\n","scorer = make_scorer(, greater_is_better=)\n","\n","\n","# TODO: Create the KNeighborsRegressor\n","neigh =\n","\n","# TODO: Create the GridSearchCV object with custom scoring\n","grid_search = GridSearchCV(\n","    ,\n","    ,\n","    scoring=,\n","    cv=5  # You can specify the number of folds for cross-validation\n",")\n","\n","# TODO: Fit the grid search to your data\n","\n","\n","# Access the best model and its parameters\n","best_model = grid_search.best_estimator_\n","best_params = grid_search.best_params_\n","\n","# Print the results\n","print(f\"Best Model: {best_model}\\n\")\n","print(f\"Best Parameters: {best_params}\\n\")\n","\n","# You can also access other grid search results\n","best_index = grid_search.best_index_\n","print('All mean test scores: \\n', grid_search.cv_results_['mean_test_score'], '\\n')\n","print('Best hyperparamter mean test scores: \\n', grid_search.cv_results_['mean_test_score'][best_index])\n"]},{"cell_type":"markdown","metadata":{"id":"-6jMVi06uxQg"},"source":["# Putting it all together in a pipeline and export pipeline\n","\n","We will now combine the preprocessing pipeline and the best model into one pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCSmBs71uxQh"},"outputs":[],"source":["# Combine the preprocessing pipeline and the best model into a new pipeline\n","pipeline = Pipeline([\n","    ('preprocess', preprocess_pipeline),\n","    ('model', best_model)\n","])"]},{"cell_type":"markdown","metadata":{"id":"wJwuisZMuxQh"},"source":["Save the pipeline using `joblib.dump` and load it back for predicting the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwD-bwinuxQh"},"outputs":[],"source":["# Save the pipeline\n","joblib.dump(pipeline, '../pipelines/house_price_pipeline.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKZqI7f_uxQh"},"outputs":[],"source":["# Load the pipeline\n","pipeline = joblib.load('../pipelines/house_price_pipeline.joblib')\n","\n","# Evaluate the pipeline on the test set\n","predict_and_print_metrics(pipeline, test_X, test_y, 'Test')\n"]},{"cell_type":"markdown","metadata":{"id":"TgCx-Rq6uxQh"},"source":["Check to see how the model performs on the test set. We will see how much percentage the predictions deviate from the actual values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRYl8yTRuxQh"},"outputs":[],"source":["percentage_diff = (pipeline.predict(test_X) - test_y)/test_y * 100\n","\n","plt.figure(figsize=(12,8))\n","plt.subplot(2,1,1)\n","plt.hist(percentage_diff, bins=100)\n","plt.xlabel('Percentage Difference')\n","plt.ylabel('Count')\n","plt.xlabel('Percentage Difference')\n","plt.xlim(-100,400)\n","plt.xticks(np.arange(-100, 400, 20))\n","\n","plt.subplot(2,1,2)\n","sns.boxplot(percentage_diff, orient='h')\n","plt.xlim(-100,400)\n","plt.xlabel('Percentage Difference')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARNXZoj4uxQh"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d23888968f03af4d8f5854645ff71495a3d8ae2aacfb1a9ca20828cd5f99507f"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}