{"cells":[{"cell_type":"markdown","metadata":{"id":"94cQohGL0vvv"},"source":["# Overview\n","This notebook is used to create a multi-class classification model that predicts the type of dry beans based on 16 features (12 dimensions and 4 shape forms). The dataset used is the \"Dry Bean Dataset\" which is available at https://archive.ics.uci.edu/dataset/602/dry+bean+dataset. Here is the UCI dataset description of the dataset:\n","\n","*Seven different types of dry beans were used in this research, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.*\n","\n","More information about the dataset can be found in the paper \"Multiclass classification of dry beans using computer vision and machine learning techniques\" by Murat Koklu and Ilker Ali Ozkan in Computers and Electronics in Agriculture, 2020.\n","\n","The dataset contains 13611 rows and 17 columns. The columns are as follows:\n","1. Area (integer, denote A): The area of a bean zone and the number of pixels within its boundaries.\n","2. Perimeter (float, denote P): Bean circumference is defined as the length of its border.\n","3. MajorAxisLength (float, denote L): The distance between the ends of the longest line that can be drawn from a bean.\n","4. MinorAxisLength (float, denote l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n","5. AspectRatio (float, denote K): Defines the relationship between MajorAxisLength and MinorAxisLength. $K = \\frac{L}{l}$\n","6. Eccentricity (float, denote Ec): Eccentricity of the ellipse having the same moments as the region.\n","7. ConvexArea (integer, denote C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n","8. EquivDiameter (float, denote Ed): Equivalent diameter. The diameter of a circle having the same area as a bean seed area. $d = \\sqrt{\\frac{4*A}{\\pi}}$\n","9. Extent (float, denote Ex): The ratio of the pixels in the bounding box to the bean area.\n","$$Ex = \\frac{A}{A_B} where A_B = AreaOfBoundingRectangle$$\n","10. Solidity (float, denote S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans. $S = \\frac{A}{C}$\n","11. Roundness (float, denote R): Calculated with the following formula: $R = \\frac{4 \\pi A}{P^2}$.\n","12. Compactness (float, denote CO): Measures the roundness of an object. $CO = \\frac{Ed}{L}$\n","13. ShapeFactor1 (float, denote SF1): Defines the relationship between MajorAxisLength and Area. $\\frac{L}{A}$\n","14. ShapeFactor2 (float, denote SF2): Defines the relationship between MinorAxisLength and Area. $\\frac{l}{A}$\n","15. ShapeFactor3 (float, denote SF3): Defines the relationship between Area and area of a circle having the same MajorAxisLength as the bean. $\\frac{A}{(\\frac{L}{2}*\\frac{L}{2}*\\pi)}$\n","16. ShapeFactor4 (float, denote SF4): Defines the relationship between Area and area of a circle having the same MinorAxisLength as the bean. $\\frac{A}{(\\frac{L}{2}*\\frac{l}{2}*\\pi)}$\n","17. Class (string): Type of a bean. Value of class variable is one of: BARBUNYA, BOMBAY, CALI, DERMASON, HOROZ, SEKER, SIRA.\n","\n","Here are some descriptions of each class, which might help us determine the features that are important for classification:\n","\n","\n","**BARBUNYA**; Beige-colored background with red stripes or variegated, speckled color, its seeds are large, physical shape is oval close to the round.\n","\n","**BOMBAY**; It is white in color, its seeds are very big and its physical structure is oval and bulging.\n","\n","**CALI**; It is white in color, its seeds are slightly plump and slightly larger than dry beans and in shape of kidney.\n","\n","**DERMASON**; This type of dry beans, which are fuller flat, is white in color and one end is round and the other ends are round.\n","\n","**HOROZ**; Dry beans of this type are long, cylindrical, white in color and generally medium in size.\n","\n","**SEKER**; Large seeds, white in color, physical shape is round.\n","\n","**SIRA**; Its seeds are small, white in color, physical structure is flat, one end is flat, and the other end is round.\n","\n","The model is created using the following steps:\n","1. Data Exploration\n","2. Data Preprocessing\n","3. Model Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"aJsmsz880vvx"},"source":["# Install Python Packages/Libraries"]},{"cell_type":"markdown","metadata":{"id":"VIU0Qe__0vvx"},"source":["Install the specified Python packages. Here's a breakdown of each package:\n","\n","`joblib`: A set of tools for pipelining Python jobs. It provides utilities for saving and loading Python objects that make it possible to save scikit-learn models in a format that can be used in production.\n","\n","`matplotlib`: A plotting library for creating visualizations in Python. It is often used in conjunction with other libraries for data analysis and machine learning.\n","\n","`numpy`: A fundamental package for scientific computing with Python. It provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays.\n","\n","`pandas`: A powerful data manipulation and analysis library. It provides data structures for efficiently storing and manipulating large datasets.\n","\n","`seaborn`: A statistical data visualization library based on Matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics.\n","\n","`scikit-learn`: A machine learning library that provides simple and efficient tools for data analysis and modeling. It includes various algorithms for classification, regression, clustering, and more.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZtyy9-20vvy"},"outputs":[],"source":["%pip install joblib==1.3.2 matplotlib==3.7.1 numpy==1.23.5 pandas==1.5.3 plotly==5.15.0 scikit-learn==1.2.2 seaborn==0.13.1 \"nbformat>=4.2.0\""]},{"cell_type":"markdown","metadata":{"id":"uMu2ylBI0vvy"},"source":["# Import Packages/Libraries"]},{"cell_type":"markdown","metadata":{"id":"QU5xQyH40vvy"},"source":["In addition to the packages/libraries installed above, we will also imported\n","\n","`typing`: A module that provides support for type hints. Type hints allow you to specify the type of a variable, function parameter, or return value. This helps improve the readability of your code and allows you to catch errors early."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31BWfnuu0vvy"},"outputs":[],"source":["import joblib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, FunctionTransformer\n"]},{"cell_type":"markdown","metadata":{"id":"MFTBKi2t0vvz"},"source":["# Data Exploration\n","\n","We want to explore the data to get a better understanding of the dataset. We will use the pandas library to load the dataset into a pandas DataFrame. A DataFrame is a two-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data, and more) in columns. It is similar to a spreadsheet or an SQL table. The DataFrame object also has powerful built-in methods for exploring and manipulating these data sets. We will first take a look at the structure of the dataset. Then, we want to explore the data to see if there are any missing values and to see if there are any skewness and outliers. We also want to see if there are any correlations between the features and the target variable. We will use the pandas library to load the dataset and the matplotlib and seaborn library to plot the data."]},{"cell_type":"markdown","metadata":{"id":"wjesAXxJ0vvz"},"source":["Load the `Dry_Bean_Dataset.csv` file into a Pandas DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"almI1TgI0vvz"},"outputs":[],"source":["csv_path = # TODO: create variable for dataset path\n","df =  # TODO: read dataset into dataframe"]},{"cell_type":"markdown","metadata":{"id":"dKnHtggT0vvz"},"source":["Display the first 5 rows of the dataframe using `head()` method of the dataframe.\n","\n","We can see that the data contains 17 columns where the first 16 columns are numerical values and the last column is a categorical value. The target variable is the `Class` column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0We6BOu10vvz"},"outputs":[],"source":["# TODO: show the first 5 rows of dataframe\n"]},{"cell_type":"markdown","metadata":{"id":"hg-npkl60vv0"},"source":["Obtain a concise summary of the dataframe. The `info()` method provides information about the dataframe, including the index range, the data types of each column, the number of non-null values, and memory usage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KjI0cnzr0vv0"},"outputs":[],"source":["# TODO: show the info of dataframe\n"]},{"cell_type":"markdown","metadata":{"id":"F_h0exN60vv0"},"source":["we can retrieve the columns of the dataframe using the `columns` attribute of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRLY_vRy0vv0"},"outputs":[],"source":["# TODO: show the columns of dataframe\n"]},{"cell_type":"markdown","metadata":{"id":"tIeBX3eN0vv0"},"source":["Retrieve the dimensions (rows, columns) of the dataframe using the `shape` attribute of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUfLw5Ag0vv1"},"outputs":[],"source":["# TODO: retrieve the number of rows and columns\n"]},{"cell_type":"markdown","metadata":{"id":"UryCe84I0vv1"},"source":["Check and handles missing values in the Pandas DataFrame.\n","\n","We see that their is no missing values in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRFBaP-p0vv1"},"outputs":[],"source":["# TODO: check the number of missing values in each column (fill in the blank)\n","print('Missing values in each column:\\n', _________________, sep='')\n","print('Original data shape: ', df.shape)"]},{"cell_type":"markdown","metadata":{"id":"JaJTQZzj0vv1"},"source":["Check the number of unique values in each column of dataframe using the `nunique()` method of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIVzmVA70vv1"},"outputs":[],"source":["# TODO: check the number of unique values in each column\n"]},{"cell_type":"markdown","metadata":{"id":"lVrQ2nFg0vv1"},"source":["We will create a list to keep track of the feature columns and a string variable for the target column. This will be useful when we visualize the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5yrHZGP0vv1"},"outputs":[],"source":["# Create a variable to store the target column name\n","target = 'Class'\n","\n","# Create a list to keep track of the feature columns\n","all_features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n","       'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n","       'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n","       'ShapeFactor3', 'ShapeFactor4']"]},{"cell_type":"markdown","metadata":{"id":"KIzfOrtY0vv2"},"source":["We will display pie charts to visualize the distribution of the target `Class` column using the `plot.pie()` method of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96tfWS0o0vv2"},"outputs":[],"source":["# TODO: set the figure size and background color to white in case your editor GUI uses a dark theme (fill in the blank)\n","plt.figure(figsize=______, facecolor=_______)\n","\n","# TODO: show the percentage of each type of bean using a pie chart\n","# plot.pie() plots a pie chart on the figure attached to the current cell\n","# autopct='%1.2f%%' shows the percentage of each category with 2 decimal places\n","\n","plt.ylabel('')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ofoF9m0V0vv2"},"source":["We will use histograms to visualize the distribution of numerical feature columns using the `hist()` method of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOUmkAKk0vv2"},"outputs":[],"source":["# Display the distribution of each feature\n","df[all_features].hist(figsize=(10, 10), bins=100)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MxuyoRZh0vv2"},"source":["Check for correlation between the features using the `corr()` method of the dataframe. The `corr()` method computes pairwise correlation of columns, excluding NA/null values. The correlation coefficient ranges from -1 to 1. When it is close to 1, it means that there is a strong positive correlation; when the coefficient is close to -1, it means that there is a strong negative correlation; when it is close to zero, it means that there is no linear correlation.\n","\n","Notice that the `Class` column is not included in the correlation matrix because it is a categorical value.\n","\n","We will look for highly correlated features and determine which features to keep and which to drop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3Nfd1Tw0vv2"},"outputs":[],"source":["# TODO: create a figure\n","\n","\n","# TODO: compute the correlation matrix using the `.corr()` method (fill in the blank)\n","correlation_matrix = _________\n","\n","# Create a mask to block the upper triangle of the correlation matrix\n","# as it is a mirror image of the lower triangle\n","# use `np.triu()` to create an upper triangle matrix of 1s and 0s\n","mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n","\n","# TODO: plot the heatmap using seaborn's `heatmap()` function\n","# set `annot=True` to show the correlation values on the heatmap\n","# set `fmt='.2f'` to round the correlation values to 2 decimal places\n","# set `mask=mask` to block the upper triangle of the correlation matrix\n","\n","\n","# show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"T9dmmRfo0vv2"},"source":["A violin plot is used to visualize the distribution of the data and its probability density. This chart is a combination of a Box Plot and a Density Plot that is rotated and placed on each side, to show the distribution shape of the data. We will use the `violinplot()` method of the seaborn library to plot the violin plot.\n","\n","We will use the violin plot to help us determine which features to keep and which to drop. We will drop the features that have similar distributions for all the classes. We will keep the features that have different distributions for different classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cww6Ynm80vv2"},"outputs":[],"source":["# TODO: sns.violinplot() plots a violin plot on the figure attached to the current cell (fill in the blank)\n","# x=label specifies the column to use for x axis\n","# y=feature specifies the column to use for y axis\n","# hue=label specifies the column to use for grouping\n","for feature in ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation',\n","                'Eccentricity', 'Extent', 'Solidity', 'roundness', 'ShapeFactor1', 'ShapeFactor2',\n","                'ShapeFactor3', 'ShapeFactor4']:\n","    sns.violinplot(________, _________, hue=target, data=df)\n","    plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OGm6kYfN0vv2"},"source":["# Data Preprocessing\n","\n","Data splitting. We are given one CSV file containing all the data. We will split the data into training, validation, and test sets.\n","- The training dataset is used to train the machine learning model. The model learns the patterns and relationships within the data from this set. The model adjusts its parameters to minimize the difference between its predictions and the ground truth values provided with this dataset.\n","- The validation dataset is used to assess the model's performance during training and guide the adjustment of the hyperparameters. Hyperparameters are training/model configurations that the programmer can manually adjust. Since the validation dataset is an independent dataset not used in directly tuning the model's internal parameters, we can use it to assess whether a model is under-fitting or over-fitting.\n","- The test dataset is kept separate from both the training and validation sets. It is used to assess the final performance of the trained model on unseen data. The test set provides an unbiased evaluation of the model's generalization to new, previously unseen examples.\n","\n","Building pipelines to preprocess the data. The preprocessing pipeline provides a systematic and efficient way to streamline and automate the data preprocessing steps. It ensures consistent application of preprocessing steps to training, validation, and test data, enhancing model reproducibility, readability, and ease of deployment. In this section, we will build a preprocessing pipeline to perform the following steps:\n"," - Use log transformation to reduce skewness\n"," - Standardize the data using StandardScaler\n"," - drop low correlation features"]},{"cell_type":"markdown","metadata":{"id":"7YBA0wo80vv3"},"source":["Reload the data into a Pandas DataFrame to reset the changes made to the dataframe in the previous section. Additionally, we will declare variables to store all feature columns and the label (target) column. We will use these variables to select the columns from the dataframe in the preprocessing pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9ARQ5yR0vv3"},"outputs":[],"source":["# TODO: reload the dataset\n","csv_path =\n","df =\n","\n","# Declare the feature columns and target column\n","label = 'Class'\n","all_features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n","       'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n","       'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n","       'ShapeFactor3', 'ShapeFactor4']"]},{"cell_type":"markdown","metadata":{"id":"i2hX7xs-0vv3"},"source":["Split the data into training, validation, and test sets. The training set will be used to train the model, the validation set will be used to evaluate the model during the training process, and the test set will be used to test the final model.\n","\n","We will be using Scikit-Learn's `train_test_split` function to split the data into training, validation, and test sets. The `train_test_split` function takes in the dataframe as argument and returns the training set and test set. We will further split the training set into training and validation sets. The final split is 60% training, 20% validation, and 20% test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKXL9o-O0vv3"},"outputs":[],"source":["# Use a random seed so that we can reproduce the results\n","# this is important when you want to compare different models\n","random_seed = 42\n","\n","# TODO: split the data into training and test sets. 80/20 split.\n","train_set, test_set =\n","\n","# TODO: split the data into training and validation sets.\n","# 75/25 split of the training set, which is 60/20 of the original set.\n","train_set, valid_set ="]},{"cell_type":"markdown","metadata":{"id":"qPgJ6LoY0vv3"},"source":["Separate the feature columns from the label column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCVAcbIj0vv3"},"outputs":[],"source":["train_X, train_y = _________[____________], _________[_____]\n","valid_X, valid_y = _________[____________], _________[_____]\n","test_X, test_y = ________[____________], ________[_____]"]},{"cell_type":"markdown","metadata":{"id":"EJBG2pSa0vv3"},"source":["Check the feature distribution in the training, validation, and test sets. We want to ensure that the label distribution is similar in all the sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjPNe7LS0vv3"},"outputs":[],"source":["# Check the distribution of the training set, validation set, and test set\n","# For each feature, overlay the histograms of the three sets on the same subplot\n","\n","# TODO: create a figure with 3 rows and 6 columns\n","fig, axes =\n","\n","# For each feature, plot the histograms of the three sets on the same subplot\n","for ax, col in zip(axes.flat, all_features):\n","    ax.hist(train_X[col], bins=50, alpha=0.2, label='train')\n","    ax.hist(valid_X[col], bins=50, alpha=0.4, label='valid')\n","    ax.hist(test_X[col], bins=50, alpha=0.3, label='test')\n","    ax.set_title(col)\n","\n","# Add a legend to the figure anchor to the top right corner\n","handles, labels = ax.get_legend_handles_labels()\n","fig.legend(handles, labels, bbox_to_anchor=(1.05, 0.95))\n","\n","# TODO: use `plt.tight_layout()` to adjust the padding between and around subplots.\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"LOS_ny170vv4"},"source":["Check the label distribution in the training, validation, and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2XoHjh70vv4"},"outputs":[],"source":["# TODO: check the label distribution of the training set, validation set, and test set\n","# overlay the histograms of the three sets (fill in the blank)\n","\n","plt.figure(figsize=(10, 5))\n","plt.hist(_______, bins=50, alpha=0.2, label=_______)\n","plt.hist(_______, bins=50, alpha=0.4, label=_______)\n","plt.hist(______, bins=50, alpha=0.3, label=______)\n","plt.title(label)\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"38ieKAv_0vv4"},"source":["## Create Preprocessing Pipeline"]},{"cell_type":"markdown","metadata":{"id":"g_KsJVOc0vv4"},"source":[" - Use log transformation to reduce skewness\n"," - Standardize the data using StandardScaler\n"," - drop low correlation features"]},{"cell_type":"markdown","metadata":{"id":"-cUmXBQD0vv4"},"source":["`ColumnTransformer` allows us to apply different preprocessing steps to different columns in the dataset. It is particularly useful when we have a dataset when we want to apply specific transformations to specific subsets of features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc-Ri59O0vv4"},"outputs":[],"source":["# Create log_transformer to apply log transformation to the data\n","# using `FunctionTransformer()` and `np.log()` function\n","log_transformer = FunctionTransformer(np.log, validate=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKOD1sur0vv4"},"outputs":[],"source":["# parameters for the preprocess pipelines\n","# A list of features to apply for log transformation and standardization\n","# Another list of features to apply for standardization\n","log_scale_features = ['Area']\n","scale_features = ['AspectRation', 'Eccentricity',  'roundness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3']\n","\n","# Create a preprocess pipeline\n","# use `ColumnTransformer()` to apply the log_transformer and StandardScaler() to the features\n","#  - use `log_std_scaler` as the name for the log_transformer and StandardScaler() pipeline\n","#  - use `std_scaler` as the name for the StandardScaler() only pipeline\n","# use `remainder='drop'` to drop the columns that are not specified in the lists above\n","preprocess_pipeline = ColumnTransformer(\n","    [\n","        ('log_std_scaler', Pipeline([('log_scaler', log_transformer),\n","                                     ('std_scaler', StandardScaler())]\n","                                   ),  log_scale_features),\n","        (\"std_scaler\", StandardScaler(), scale_features),\n","    ],\n","    remainder=\"drop\")"]},{"cell_type":"markdown","metadata":{"id":"jai2JX840vv4"},"source":["Use the full pipeline to transform the training and validation data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gp_-bHN0vv4"},"outputs":[],"source":["# TODO: fit the pipeline to the training set and transform it\n","train_X =\n","\n","# TODO: transform the validation\n","valid_X ="]},{"cell_type":"markdown","metadata":{"id":"KsBLEAqa0vv4"},"source":["# Train and Evaluate Models"]},{"cell_type":"markdown","metadata":{"id":"7UlTcXqI0vv4"},"source":["This perhaps is the easier part of the workflow in terms of coding. Scikit-learn provides a wide range of machine learning tools that we can use to train and evaluate our data. Their APIs are very consistent. We can use the same code to train and evaluate different models by simply changing the model class."]},{"cell_type":"markdown","metadata":{"id":"YNp8nQ7a0vv4"},"source":["Before we start, we will implement an evaluation function. This function `predict_and_print_metrics` takes a multiclass classifier model, input features (X), target values (y), and a dataset name. It then predicts the target values using the model, calculates and prints the accuracy and confusion matrix.\n","\n","Accuracy computes as the proportion of true results ($y_i == \\hat{y_i}$) among the total number of samples examined. It is calculated as $\\frac{\\sum_i^N(y_i == \\hat{y_i})}{total\\_num\\_samples}$.\n","\n","Confusion matrix is a table that summarizes the performance of a classification model. It compares the actual values with the predicted values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbUe2n4l0vv4"},"outputs":[],"source":["#TODO: import the accuracy_score and confusion_matrix functions (fill in the blank)\n","\n","\n","def plot_confusion_matrix(cm: np.ndarray, classes: list, cmap=plt.cm.Blues) -> None:\n","    \"\"\"\n","    This function plots the confusion matrix using seaborn.\n","    \"\"\"\n","    plt.figure(figsize=(5, 5))\n","    sns.heatmap(cm, annot=True, cmap=cmap, fmt='g', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted label')\n","    plt.ylabel('True label')\n","    plt.show()\n","\n","# TODO: create a function to predict the target values and print the metrics\n","# include accuracy and confusion matrix\n","def predict_and_print_metrics(model, X: np.ndarray, y: np.ndarray, name: str) -> None:\n","    # predict the target values\n","    y_pred =\n","\n","    # compute accuracy\n","    accuracy =\n","\n","    # compute confusion matrix\n","    classes = ['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA']\n","    cm =\n","\n","    print(f\"Model: {name}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","\n","    print(f\"Confusion Matrix:\\n\")\n","    plot_confusion_matrix(cm, classes)"]},{"cell_type":"markdown","metadata":{"id":"DkgBNG5D0vv5"},"source":["Train a Decision Tree model. The training can be done in three lines of code. First, we import the model class from the Scikit-Learn library. Then, we create an instance of the model class. Third, we call the `fit` method of the model instance to train the model. The `fit` method takes in the training features and labels as arguments. The model learns the patterns and relationships within the data. Then, we use the trained model to predict the target values for the training and validation set, and we evaluate the model using the evaluation metrics mentioned above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N98a5Hl90vv5"},"outputs":[],"source":["# TODO: import the decision tree classifier\n","\n","\n","# TODO: instantiate and train the model\n","tree_clf =\n","tree_clf.fit(train_X, train_y)\n","\n","# TODO: evaluate the model on the training set and validation set\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O_HkxuLz0vv5"},"source":["We will train and evaluate other models (K-Nearest Neighbors Classifier and Logistic Regression). We will use the same code to train and evaluate different models by simply changing the model class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i602KcxU0vv5"},"outputs":[],"source":["# Import the k-nearest neighbors classifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# TODO: instantiate and train the model\n","neigh =\n","neigh.fit(train_X, train_y)\n","\n","# TODO: evaluate the model on the training set and validation set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48DBNDY_0vv5"},"outputs":[],"source":["# Import the logistic regression classifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# TODO: instantiate and train the model\n","log_reg =\n","log_reg.fit(train_X, train_y)\n","\n","# TODO: evaluate the model on the training set and validation set\n"]},{"cell_type":"markdown","metadata":{"id":"P-JnBvcc0vv5"},"source":["In the case where the model has hyperparameters, we can either write our own code to search for the best hyperparameters or use Scikit-Learn's `GridSearchCV` class to search for the best hyperparameters. We start off by writing our own code to search for the best hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pX-MQJV60vv5"},"outputs":[],"source":["# train a decision tree classifier with search for the best hyperparameters\n","\n","train_scores = []\n","valid_scores = []\n","depths = range(2,30,2)\n","for depth in depths:\n","    tree_clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n","    tree_clf.fit(train_X, train_y)\n","\n","    # evaluate the model on the train set\n","    train_y_pred = tree_clf.predict(train_X)\n","    train_acc = accuracy_score(train_y, train_y_pred)\n","    train_scores.append(train_acc)\n","\n","    # evaluate the model on the validation set\n","    valid_y_pred = tree_clf.predict(valid_X)\n","    valid_acc = accuracy_score(valid_y, valid_y_pred)\n","    valid_scores.append(valid_acc)\n","\n","# plot the learning curves\n","plt.plot(depths, train_scores, label='train')\n","plt.plot(depths, valid_scores, label='valid')\n","plt.xlabel('max_depth')\n","plt.ylabel('accuracy')\n","plt.legend()\n","\n","# print the depths and the corresponding scores\n","for depth, train_score, valid_score in zip(depths, train_scores, valid_scores):\n","    print(f'depth: {depth}, train score: {train_score}, valid score: {valid_score}')"]},{"cell_type":"markdown","metadata":{"id":"klROzF4m0vv5"},"source":["Use `GridSearchCV` to search for the best hyperparameters. `GridSearchCV` takes in the model class, hyperparameter grid, evaluation metric (scorer), and the number of folds as arguments. It then searches for the best hyperparameters based on the evaluation metric. We will use the same evaluation metric as before, accuracy. We will combine the training and evaluation data into one dataset. This is because `GridSearchCV` uses cross-validation to evaluate the model. We will use 5-fold cross-validation, which means that the dataset will be split into 5 folds. The model will be trained and evaluated 5 times, each time using a different fold as the evaluation set. The final evaluation metric will be the average of the 5 evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUJSbrZs0vv5"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer\n","\n","# Combine the train and validation sets for GridSearchCV\n","train_valid_X = np.concatenate([train_X, valid_X])\n","train_valid_y = np.concatenate([train_y, valid_y])\n","\n","# TODO: Specify the parameter grid for the grid search (fill in the blank)\n","param_grid = {\n","    'max_depth': range(_,__,_),\n","}\n","\n","\n","# TODO: Make the metric a scorer using make_scorer (fill in the blank)\n","scorer = make_scorer(accuracy_score, greater_is_better=____)\n","\n","\n","# TODO: Create the DecisionTreeClassifier\n","tree_clf =\n","\n","# TODO: Create the GridSearchCV object with custom scoring (fill in the blank)\n","grid_search = GridSearchCV(\n","    ________,\n","    __________,\n","    scoring=______,\n","    cv=5  # You can specify the number of folds for cross-validation\n",")\n","\n","# TODO: Fit the grid search to your data\n","\n","\n","# Access the best model and its parameters\n","best_model = grid_search.best_estimator_\n","best_params = grid_search.best_params_\n","\n","# Print the results\n","print(f\"Best Model: {best_model}\")\n","print(f\"Best Parameters: {best_params}\")\n","\n","# You can also access other information like grid search results, etc.\n","best_index = grid_search.best_index_\n","print(grid_search.cv_results_['mean_test_score'])"]},{"cell_type":"markdown","metadata":{"id":"-3cqWMW40vv5"},"source":["# Putting it all together in a pipeline and export pipeline\n","\n","We will now combine the preprocessing pipeline and the best model into one pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH2HN3ux0vv5"},"outputs":[],"source":["# TODO: combine the preprocessing pipeline and the best model into a new pipeline\n","pipeline = Pipeline([\n","    ('preprocess', ___________________),\n","    ('model', __________)\n","])"]},{"cell_type":"markdown","metadata":{"id":"6hFjCH7f0vv5"},"source":["Save the pipeline using `joblib.dump` and load it back for predicting the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrKsNW9K0vv5"},"outputs":[],"source":["# TODO: save the pipeline\n","joblib.dump(________, ____________________________________)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FEIucvT0vv6"},"outputs":[],"source":["# TODO: load the pipeline\n","pipeline = joblib.load(____________________________________)\n","\n","# TODO: evaluate the pipeline on the test set\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zskvmOl0vv6"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d23888968f03af4d8f5854645ff71495a3d8ae2aacfb1a9ca20828cd5f99507f"}},"colab":{"provenance":[{"file_id":"1aD6tDl6wKgvpvGzPHngln1EQborZL6zN","timestamp":1727940118301},{"file_id":"1tuXqr8ctf3eW1OT16eQ-5j2pKGxyySaI","timestamp":1727933075529}]}},"nbformat":4,"nbformat_minor":0}