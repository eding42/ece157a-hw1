{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook is used to create a binary classification model that predicts  whether an individual has (pre)diabetes or not based on features regarding their lifestyle and health measurements. The dataset used is the \"Diabetes Health Indicators Dataset\" which is available at https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_binary_5050split_health_indicators_BRFSS2015.csv. Here is a summary of the Kaggle description of the dataset:\n",
    "\n",
    "The dataset was originally collected and released by the Behavioral Risk Factor Surveillance System (BRFSS) from the Centers for Disease Control and Prevention (CDC). The original dataset is available in this Kaggle project (https://www.kaggle.com/datasets/cdc/behavioral-risk-factor-surveillance-system). The BRFSS is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. \n",
    "\n",
    "This dataset was derived from the 2015 BRFSS survey. The original dataset contains 441455 individuals and has 330 features, where the features are either questions directly asked of participants, or calculated variables based on individual participant responses.\n",
    "\n",
    "For this notebook, we are using the `diabetes_binary_5050split_health_indicators_BRFSS2015.csv` dataset, which is a clean dataset of 70692 survey responses to the CDC's BRFSS2015. It has an equal 50-50 split of respondents with no diabetes and with either prediabetes or diabetes. The target variable Diabetes_binary has 2 classes: 0 is for no diabetes, and 1 is for prediabetes or diabetes. This dataset has 21 feature variables and is balanced. The notebook used by the Kaggle project owner to create this dataset is available at https://www.kaggle.com/code/alexteboul/diabetes-health-indicators-dataset-notebook/notebook.\n",
    "\n",
    "The dataset contains 70692 rows and 22 columns. The columns are as follows:\n",
    "\n",
    "1. Diabetes_binary (float, target): 0 = no diabetes, 1 = prediabetes or diabetes\n",
    "2. HighBP (float): 0 = no high blood pressure, 1 = high blood pressure\n",
    "3. HighChol (float): 0 = no high cholesterol, 1 = high cholesterol\n",
    "4. CholCheck (float): 0 = no cholesterol check in 5 years, 1 = yes cholesterol check in 5 years\n",
    "5. BMI (float): Body Mass Index\n",
    "6. Smoker (float): Have you smoked at least 100 cigarettes in your entire life? 0 = no, 1 = yes\n",
    "7. Stroke (float): Ever told you had a stroke. 0 = no, 1 = yes\n",
    "8. HeartDiseaseorAttack (float): Coronary heart disease (CHD) or myocardial infarction (MI). 0 = no, 1 = yes\n",
    "9. PhysActivity (float): Physical activity in past 30 days - not including job. 0 = no, 1 = yes\n",
    "10. Fruits (float): Consume fruit one or more times per day. 0 = no, 1 = yes\n",
    "11. Veggies (float): Consume vegetables one or more times per day. 0 = no, 1 = yes\n",
    "12. HvyAlcoholConsump (float): Adult men drink >=14 drinks per week, or adult women drink >=7 drinks per week. 0 = no, 1 = yes\n",
    "13. AnyHealthcare (float): Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc. 0 = no, 1 = yes\n",
    "14. NoDocbcCost (float): Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no, 1 = yes\n",
    "15. GenHlth (float): Would you say that in general your health is: scale 1-5. \n",
    "- 1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor\n",
    "16. MentHlth (float): Days of poor mental health scale 1-30 days.\n",
    "17. PhysHlth (float): Physical illness or injury days in past 30 days scale 1-30.\n",
    "18. DiffWalk (float): Do you have serious difficulty walking or climbing stairs? 0 = no, 1 = yes\n",
    "19. Sex (float): 0 = female 1 = male\n",
    "20. Age (float): 13-level age category. \n",
    "- 1 = 18-24, 2 = 25-29, 3 = 30-34, 4 = 35-39, 5 = 40-44, 6 = 45-49, 7 = 50-54, 8 = 55-59, 9 = 60-64, 10 = 65-69, 11 = 70-74, 12 = 75-76, 13 = 80 or older\n",
    "21. Education (float): Education level scale 1-6. \n",
    "- 1 = never attended school or only kindergarten, 2 = elementary (grades 1 - 8), 3 = some high school (grades 9 - 11), 4 = high school graduate (grade 12 or GED), 5 = some college or technical school (College 1 year to 3 years), 6 = college graduate (college 4 years or more)\n",
    "22. Income (float): Income scale scale 1-8. \n",
    "- 1 = less than $10,000, 2 = $10,000 t0 $15,000, 3 = $15,000 to $20,000, 4 = $20,000 to $25,000, 5 = $25,000 to $35,000, 6 = $35,000 to $50,000, 7 = $50,000 to $75,000, 8 = $75,000 or more\n",
    "\n",
    "\n",
    "This notebook performs the following steps to create a classification model:\n",
    "1. Data Exploration\n",
    "2. Data Preprocessing\n",
    "3. Model Training and Evaluation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Python Packages/Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the specified Python packages. Here's a breakdown of each package:\n",
    "\n",
    "`joblib`: A set of tools for pipelining Python jobs. It provides utilities for saving and loading Python objects that make it possible to save scikit-learn models in a format that can be used in production.\n",
    "\n",
    "`matplotlib`: A plotting library for creating visualizations in Python. It is often used in conjunction with other libraries for data analysis and machine learning.\n",
    "\n",
    "`numpy`: A fundamental package for scientific computing with Python. It provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays.\n",
    "\n",
    "`pandas`: A powerful data manipulation and analysis library. It provides data structures for efficiently storing and manipulating large datasets.\n",
    "\n",
    "`seaborn`: A statistical data visualization library based on Matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics.\n",
    "\n",
    "`scikit-learn`: A machine learning library that provides simple and efficient tools for data analysis and modeling. It includes various algorithms for classification, regression, clustering, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install joblib==1.3.2 matplotlib==3.7.1 numpy==1.23.5 pandas==1.5.3 plotly==5.15.0 scikit-learn==1.2.2 seaborn==0.13.1 \"nbformat>=4.2.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages/Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the packages/libraries installed above, we will also imported\n",
    "\n",
    "`typing`: A module that provides support for type hints. Type hints allow you to specify the type of a variable, function parameter, or return value. This helps improve the readability of your code and allows you to catch errors early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "We want to explore the data to get a better understanding of the dataset. We will use the pandas library to load the dataset into a pandas DataFrame. A DataFrame is a two-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data, and more) in columns. It is similar to a spreadsheet or an SQL table. The DataFrame object also has powerful built-in methods for exploring and manipulating these data sets. We will first take a look at the structure of the dataset. Then, we want to explore the data to see if there are any missing values and to see if there are any skewness and outliers. We also want to see if there are any correlations between the features and the target variable. We will use the pandas library to load the dataset and the matplotlib and seaborn library to plot the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `diabetes_binary_5050split_health_indicators_BRFSS2015.csv` file into a Pandas DataFrame using the `read_csv()` function. The `read_csv()` function takes in the path to the CSV file as a parameter and returns a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create variable for dataset path\n",
    "csv_path = (\n",
    ")\n",
    "# TODO: read dataset into dataframe\n",
    "df = (\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 5 rows of the dataframe using `head()` method of the dataframe.\n",
    "\n",
    "We can see that the data contains some of the columns where some are binary (1s and 0s) and others are floating points. The target variable is the `Diabetes_binary` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show the first 5 rows of dataframe\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a concise summary of the dataframe. The `info()` method provides information about the dataframe, including the index range, the data types of each column, the number of non-null values, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show the info of dataframe\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can retrieve the columns of the dataframe using the `columns` attribute of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show the columns of dataframe\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the dimensions (rows, columns) of the dataframe using the `shape` attribute of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: retrieve the number of rows and columns\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and handles missing values in the Pandas DataFrame.\n",
    "\n",
    "We see that their is no missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the number of missing values in each column\n",
    "print('Missing values in each column:\\n', (\n",
    "    ), sep='')\n",
    "print('Original data shape: ', df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of unique values in each column of dataframe using the `nunique()` method of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the number of unique values in each column\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though many of the columns are numerical values, they are actually binary or categorical values. We will create lists to keep track of the numerical columns, binary columns, categorical columns, and label column. This will be useful when we visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create list of column names for each data type\n",
    "label = [\n",
    "]\n",
    "bin_cols = [\n",
    "]\n",
    "cat_cols = [\n",
    "]\n",
    "num_cols = [\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display pie charts to visualize the distribution of the target `Diabetes_binary` column using the `plot.pie()` method of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set the figure size and background color to white in case your editor GUI uses a dark theme\n",
    "plt.figure(\n",
    "    figsize=(\n",
    "    ),\n",
    "    facecolor=(\n",
    "    ),\n",
    ")\n",
    "\n",
    "# TODO: show the percentage of each category using a pie chart\n",
    "# plot.pie() plots a pie chart on the figure attached to the current cell\n",
    "# autopct='%1.2f%%' shows the percentage of each category with 2 decimal places\n",
    "\n",
    "plt.ylabel(label, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display pie charts to visualize the distribution of binary feature columns using the `plot.pie()` method of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set the figure size and background color to white in case your editor GUI uses a dark theme\n",
    "fig, axes = plt.subplots(nrows=2, ncols=7,\n",
    "    figsize=(\n",
    "    ),\n",
    "    facecolor=(\n",
    "    ),\n",
    ")\n",
    "\n",
    "# TODO: iterate over each column name and its corresponding axes object\n",
    "# to plot the percentage of each binary feature using a pie chart\n",
    "for ax, col in zip(axes.flatten(), (\n",
    "    # Hint: What is the variable where we put the binary columns?\n",
    ")):\n",
    "    # TODO: show the percentage of each category using a pie chart\n",
    "    # plot.pie() plots a pie chart on the figure attached to the current cell\n",
    "    # autopct='%1.2f%%' shows the percentage of each category with 2 decimal places\n",
    "",
    "    ax.set_ylabel(col, fontsize=14)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize the distribution of the categorical feature columns by using the `plot.bar()` method of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set the figure size and background color to white in case your editor GUI uses a dark theme\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3,",
    "    figsize=(\n",
    "    ),\n",
    "    facecolor=(\n",
    "    ),\n",
    ")\n",
    "\n",
    "# TODO: iterate over each column name and its corresponding axes object\n",
    "# to plot the bar plot of each categorical column\n",
    "for ax, col in zip(axes.flatten(), (\n",
    "    # Hint: What is the variable where we put the categorical columns?\n",
    ")):\n",
    "    #TODO: plot the bar plot of how often each categorical column appears\n",
    "\n",
    "    ax.set_ylabel('Count', fontsize=14)\n",
    "    ax.set_xlabel(col, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate descriptive statistics of the numerical columns in the dataframe using the `describe()` method of the dataframe. The `describe()` method computes summary statistics of numerical columns, including count, mean, standard deviation, minimum, maximum, and the 25, 50, and 75 percentiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get the summary statistics of the numerical columns\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution of the numerical columns in the dataframe using the `hist()` method of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the distribution for all numerical columns\n",
    "# bins=50 means to divide the range of values into 50 equal-sized bins\n",
    "# figsize=(5, 5) means to set the figure size to width=5 and height=5\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for correlation between the features and the target variable using the `corr()` method of the dataframe. The `corr()` method computes pairwise correlation of columns, excluding NA/null values. The correlation coefficient ranges from -1 to 1. When it is close to 1, it means that there is a strong positive correlation; when the coefficient is close to -1, it means that there is a strong negative correlation; when it is close to zero, it means that there is no linear correlation.\n",
    "\n",
    "The correlation matrix is quite large and a bit difficult to read. We will look at the sorted dataframe `Diabetes_binary` column instead in the next cells.\n",
    "\n",
    "But, at a glance, we can see that the `Diabetes_binary` column has a relatively higher correlation with the `HighBP`, `HighChol`, `BMI`, `GenHlth`, `DiffWalk`,  and `Age` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a figure \n",
    "\n",
    "# TODO: compute the correlation matrix using the `.corr()` method\n",
    "correlation_matrix = (\n",
    ")\n",
    "\n",
    "# TODO: create a mask to block the upper triangle of the correlation matrix\n",
    "# as it is a mirror image of the lower triangle\n",
    "# use `np.triu()` to create an upper triangle matrix of True and False\n",
    "mask = (\n",
    ")\n",
    "\n",
    "# TODO: plot the heatmap using seaborn's `heatmap()` function\n",
    "# set `annot=True` to show the correlation values on the heatmap\n",
    "# set `fmt='.2f'` to round the correlation values to 2 decimal places\n",
    "# set `mask=mask` to block the upper triangle of the correlation matrix\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix can also be viewed as a dataframe to see the correlation coefficients of all the features with each other. Execute the cell below to view the correlation matrix as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the correlation coefficients of the features with the target variable by selecting the column named `Diabetes_binary` from the correlation matrix dataframe. We have sorted that column in descending order to see the features with the highest correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_corr = correlation_matrix['Diabetes_binary'].sort_values(ascending=False)\n",
    "print(diabetes_corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot cluster bar charts to visualize the correlation between the binary/categorical features and the target variable using the `countplot()` method of the seaborn library. The `countplot()` method shows the counts of observations in each categorical bin using bars. The x-axis represents the feature and the y-axis represents the count of observations. The color of the bars represents the target variable. We will look at features with at least 0.1 (absolute) correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['GenHlth', 'HighBP', 'HighChol', 'Age', 'DiffWalk', 'PhysHlth', 'HeartDiseaseorAttack',\n",
    "            'Stroke', 'CholCheck', 'PhysActivity', 'Education', 'Income']\n",
    "\n",
    "# TODO: create a figure with 3 rows and 4 columns of subplots\n",
    "fig, axes = (\n",
    ")\n",
    "\n",
    "# TODO: iterate over each feature and its corresponding axes object\n",
    "# to plot the countplot of each feature\n",
    "for ax, feature in zip(axes.flatten(), (\n",
    ")):\n",
    "    # TODO: plot the countplot of the current feature\n",
    "",
    "    ax.get_legend().remove()\n",
    "\n",
    "# TODO: show the legend anchored to the upper center\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor= \n",
    ")\n",
    "\n",
    "# TODO: set the figure tight layout\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the numerical features, we will use a similar plot that uses a histogram to visualize the correlation between the numerical features and the target variable. We will use the `histplot()` method of the seaborn library. The `histplot()` method plots univariate histograms to show distributions of datasets. The x-axis represents the feature and the y-axis represents the count of observations. The color of the bars represents the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['PhysHlth', 'BMI']\n",
    "\n",
    "# TODO: create subplots with 1 row and 2 columns of subplots\n",
    "fig, axes = (\n",
    ")\n",
    "\n",
    "# TODO: iterate over each column name and its corresponding axes object\n",
    "# to plot the histogram of each numerical column\n",
    "for ax, feature in zip(axes.flatten(), (\n",
    ")):\n",
    "    #TODO: Create a histogram of this feature's column.",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data splitting. We are given one CSV file containing all the data. We will split the data into training, validation, and test sets. \n",
    "- The training dataset is used to train the machine learning model. The model learns the patterns and relationships within the data from this set. The model adjusts its parameters to minimize the difference between its predictions and the ground truth values provided with this dataset.\n",
    "- The validation dataset is used to assess the model's performance during training and guide the adjustment of the hyperparameters. Hyperparameters are training/model configurations that the programmer can manually adjust. Since the validation dataset is an independent dataset not used in directly tuning the model's internal parameters, we can use it to assess whether a model is under-fitting or over-fitting. \n",
    "- The test dataset is kept separate from both the training and validation sets. It is used to assess the final performance of the trained model on unseen data. The test set provides an unbiased evaluation of the model's generalization to new, previously unseen examples. \n",
    "\n",
    "Building pipelines to preprocess the data. The preprocessing pipeline provides a systematic and efficient way to streamline and automate the data preprocessing steps. It ensures consistent application of preprocessing steps to training, validation, and test data, enhancing model reproducibility, readability, and ease of deployment. In this section, we will build a preprocessing pipeline to perform the following steps:\n",
    " - Standardize the data using StandardScaler\n",
    " - Keep selected features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the data into a Pandas DataFrame to reset the changes made to the dataframe in the previous section. Additionally, we will declare variables to store all the feature columns and the label (target) columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reload the dataset\n",
    "csv_path = (\n",
    ")\n",
    "df = \n",
    "\n",
    "# TODO: create variables to keep track of the features and labels\n",
    "all_features = [\n",
    "]\n",
    "\n",
    "label = (\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training, validation, and test sets. The training set will be used to train the model, the validation set will be used to evaluate the model during the training process, and the test set will be used to test the final model.\n",
    "\n",
    "We will be using Scikit-Learn's `train_test_split` function to split the data into training, validation, and test sets. The `train_test_split` function takes in the dataframe as argument and returns the training set and test set. We will further split the training set into training and validation sets. The final split is 60% training, 20% validation, and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use a random seed so that we can reproduce the results\n",
    "# this is important when you want to compare different models in different executions\n",
    "random_seed = 42\n",
    "\n",
    "# TODO: split the data into training and test sets. 80/20 split.\n",
    "train_set, test_set = (\n",
    ")\n",
    "\n",
    "# TODO: split the data into training and validation sets. \n",
    "# 75/25 split of the training set, which is 60/20 of the original set.\n",
    "train_set, valid_set = (\n",
    "),"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the feature columns from the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: separate the features and labels in the training, validation, and test sets\n",
    "train_X, train_y = \n",
    "valid_X, valid_y = \n",
    "test_X, test_y = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the feature distribution in the training, validation, and test sets. We want to ensure that the label distribution is similar in all the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the training set, validation set, and test set\n",
    "# For each feature, overlay the histograms of the three sets on the same subplot\n",
    "\n",
    "# TODO: create a figure with 3 rows and 7 columns\n",
    "fig, axes = \n",
    "\n",
    "# TODO: for each feature, plot the histograms of the three sets on the same subplot\n",
    "for ax, col in zip(axes.flat, (\n",
    ")):\n",
    "    # TODO: Plot the histogram for this feature in each of the three sets.\n",
    "\n",
    "    ax.set_title(col)\n",
    "\n",
    "# TODO: add a legend to the figure anchor to the top right corner\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor= \n",
    ")\n",
    "\n",
    "# TODO: use `plt.tight_layout()` to adjust the padding between and around subplots.\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the label distribution in the training, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the label distribution of the training set, validation set, and test set\n",
    "# overlay the histograms of the three sets\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.title(label)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a copy of the dataframe to use later for checking column order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a copy of train_X \n",
    "train_X_df = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preprocessing Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardize the numerical data using StandardScaler\n",
    "- Keep selected features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ColumnTransformer` allows us to apply different preprocessing steps to different columns in the dataset. It is particularly useful when we have a dataset with heterogeneous data types or when we want to apply specific transformations to specific subsets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the preprocess pipelines\n",
    "num_features = ['BMI']\n",
    "passthrough_features = ['HighBP', 'HighChol',  'CholCheck', 'Stroke', \n",
    "                        'HeartDiseaseorAttack', 'PhysActivity','GenHlth',\n",
    "                        'PhysHlth', 'DiffWalk',  'Age', 'Education','Income'\n",
    "                        ]\n",
    "\n",
    "# TODO: create a preprocess pipeline\n",
    "# use StandardScaler() for the numerical features\n",
    "# use \"passthrough\" for the features to be kept as is\n",
    "preprocess_pipeline = \n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the full pipeline to transform the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit the pipeline to the training set and transform it\n",
    "train_X = \n",
    "\n",
    "# TODO: transform the validation\n",
    "valid_X = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to verify the order of the columns in the transformed data. The columns should in the same order as the process order of the column transform pipeline. \n",
    "\n",
    "The verification is done by checking the values in the columns of the array versus the columns of the dataframe. Therefore, this is only valid for passthrough columns and not for transformed columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = num_features + passthrough_features\n",
    "\n",
    "for col_index, f in enumerate(feature_names):\n",
    "    if(f == 'BMI'):\n",
    "        print(f'Skipping {f} since it is not a passthorugh column.')\n",
    "        continue\n",
    "\n",
    "    is_same = (train_X_df[f] == train_X[:, col_index]).all()\n",
    "    if(not is_same):\n",
    "        print(f'{f} and transformed column {col_index} differs')\n",
    "        break\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This perhaps is the easier part of the workflow in terms of coding. Scikit-learn provides a wide range of machine learning tools that we can use to train and evaluate our data. Their APIs are very consistent. We can use the same code to train and evaluate different models by simply changing the model class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will implement an evaluation function. This function `predict_and_print_metrics` takes a binary classification model, input features (X), target values (y), and a dataset name. It then predicts the target values using the model, calculates and prints the accuracy, confusion matrix, and a prediction summary (with precision, recall, and f1-score).\n",
    "\n",
    "Accuracy computes as the proportion of true results (both true positives and true negatives) among the total number of samples. It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "\n",
    "Precision computes the proportion of predicted positives that are correctly identified. It is calculated as TP / (TP + FP).\n",
    "\n",
    "Recall computes the proportion of actual positives that are correctly identified. It is calculated as TP / (TP + FN).\n",
    "\n",
    "F1-score is the harmonic mean of precision and recall. It is calculated as 2 * precision * recall / (precision + recall).\n",
    "\n",
    "Confusion matrix is a table that summarizes the performance of a classification model. It compares the actual values with the predicted values. It has four entries: true negatives (TN), false negatives (FN), true positives (TP), and false positives (FP). It is shown as:\n",
    "\n",
    "| TN | FP |\n",
    "|----|----|\n",
    "| FN | TP |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: import the accuracy_score, confusion_matrix, precision_score, recall_score, and f1_score functions\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, classes: list, cmap=plt.cm.Blues) -> None:\n",
    "    \"\"\"\n",
    "    This function plots the confusion matrix using seaborn.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, cmap=cmap, fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# TODO: create a function to predict the target values and print the metrics\n",
    "# include accuracy, precision, recall, f1 score, and confusion matrix\n",
    "def predict_and_print_metrics(model, X: np.ndarray, y: np.ndarray, name: str) -> None:\n",
    "    # predict the target values\n",
    "    y_pred = \n",
    "\n",
    "    # compute accuracy\n",
    "    accuracy = \n",
    "\n",
    "    # compute confusion matrix\n",
    "    cm = \n",
    "\n",
    "    # compute precision\n",
    "    precision = \n",
    "\n",
    "    # compute recall\n",
    "    recall = \n",
    "\n",
    "    # compute f1 score\n",
    "    f1 = \n",
    "\n",
    "    print(f\"{name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    print(f\"Confusion Matrix:\\n\")\n",
    "    plot_confusion_matrix(cm, ['No Diabetes', 'Diabetes'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Decision Tree Classifier model. The training can be done in three lines of code. First, we import the model class from the Scikit-Learn library. Then, we create an instance of the model class. Third, we call the `fit` method of the model instance to train the model. The `fit` method takes in the training features and labels as arguments. The model learns the patterns and relationships within the data. Then, we use the trained model to predict the target values for the training and validation set, and we evaluate the model using the evaluation metrics mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import the decision tree classifier\n",
    "\n",
    "\n",
    "# TODO: instantiate and train the model\n",
    "tree_clf = \n",
    "\n",
    "# TODO: evaluate the model on the training set and validation set\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the decision tree to examine the decision making process. Use `plot_tree` method of the classifier.\n",
    "\n",
    "Each node of the tree shows \n",
    "- The feature that is considered and the splitting value of the feature.\n",
    "- The purity or impurity of the node in terms of class values\n",
    "- Training samples that satisfy all condition leading to this node\n",
    "- Within those training samples, how many samples are in each class\n",
    "- Dominant class of this node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the decision tree to visualize the decision making process\n",
    "# use the plot_tree method of the classifier\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train and evaluate other models (K-Nearest Neighbors Classifier and Logistic Regression). We will use the same code to train and evaluate different models by simply changing the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import the k-nearest neighbors classifier\n",
    "\n",
    "\n",
    "# TODO: instantiate and train the model\n",
    "neigh = \n",
    "\n",
    "# TODO: evaluate the model on the training set and validation set\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import the logistic regression classifier\n",
    "\n",
    "\n",
    "# TODO: instantiate and train the model\n",
    "log_reg = \n",
    "\n",
    "# TODO: evaluate the model on the training set and validation set\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where the model has hyperparameters, we can either write our own code to search for the best hyperparameters or use Scikit-Learn's `GridSearchCV` class to search for the best hyperparameters. We start off by writing our own code to search for the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a decision tree classifier and search for the best hyperparameters\n",
    "\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "depths = range(2,30,2)\n",
    "for depth in depths:\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=depth, random_state=random_seed)\n",
    "    tree_clf.fit(train_X, train_y)\n",
    "\n",
    "    # evaluate the model on the train set\n",
    "    train_y_pred = tree_clf.predict(train_X)\n",
    "    train_acc = accuracy_score(train_y, train_y_pred)\n",
    "    train_scores.append(train_acc)\n",
    "\n",
    "    # evaluate the model on the validation set\n",
    "    valid_y_pred = tree_clf.predict(valid_X)\n",
    "    valid_acc = accuracy_score(valid_y, valid_y_pred)\n",
    "    valid_scores.append(valid_acc)\n",
    "\n",
    "# plot the learning curves\n",
    "plt.plot(depths, train_scores, label='train')\n",
    "plt.plot(depths, valid_scores, label='valid')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# print the depths and the corresponding scores\n",
    "for depth, train_score, valid_score in zip(depths, train_scores, valid_scores):\n",
    "    print(f'depth: {depth}, train score: {train_score}, valid score: {valid_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `GridSearchCV` to search for the best hyperparameters. `GridSearchCV` takes in the model class, hyperparameter grid, evaluation metric (scorer), and the number of folds as arguments. It then searches for the best hyperparameters based on the evaluation metric. We will use the same evaluation metric as before, accuracy. We will combine the training and evaluation data into one dataset. This is because `GridSearchCV` uses cross-validation to evaluate the model. We will use 5-fold cross-validation, which means that the dataset will be split into 5 folds. The model will be trained and evaluated 5 times, each time using a different fold as the evaluation set. The final evaluation metric will be the average of the 5 evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# TODO: combine the train and validation sets for GridSearchCV\n",
    "\n",
    "\n",
    "# TODO: Specify the parameter grid for the grid search\n",
    "param_grid = {\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: Make the metric a scorer using make_scorer\n",
    "scorer = make_scorer(\n",
    "    ,\n",
    "    greater_is_better= \n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Create the DecisionTreeClassifier\n",
    "tree_clf = \n",
    "\n",
    "# TODO: Create the GridSearchCV object with custom scoring\n",
    "grid_search = GridSearchCV(\n",
    "    ,\n",
    "    ,\n",
    "    scoring= \n",
    "    cv=5  # You can specify the number of folds for cross-validation\n",
    ")\n",
    "\n",
    "# TODO: Fit the grid search to your data\n",
    "\n",
    "\n",
    "# Access the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# You can also access other information like grid search results, etc.\n",
    "best_index = grid_search.best_index_\n",
    "print(grid_search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together in a pipeline and export pipeline\n",
    "\n",
    "We will now combine the preprocessing pipeline and the best model into one pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: combine the preprocessing pipeline and the best model into a new pipeline\n",
    "pipeline = \n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pipeline using `joblib.dump` and load it back for predicting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save the pipeline\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the pipeline\n",
    "\n",
    "\n",
    "# TODO: evaluate the pipeline on the test set\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d23888968f03af4d8f5854645ff71495a3d8ae2aacfb1a9ca20828cd5f99507f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
